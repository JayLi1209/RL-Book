`c_r` is run with N = 25, and `c_r_2` is run with N = 40. At `ts = 1000`, we shift to the right graph of Example 8.2. Both graphs exhibits great randomization errors. The book reduces the error by using random seeding to make sure both methods explores the same trajectory of grids during their first episode, but I didn't implement that. In general, reward updates are faster than action value updates for Dyna-Q+ because it encourages instantaneous exploration. 