{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_task(n_states, b, seed=None):\n",
    "    \"\"\"Generate a random episodic task.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    r_term = np.random.normal(0, 1, (n_states, 2))\n",
    "    next_states = np.zeros((n_states, 2, b), dtype=np.int32)\n",
    "    rewards = np.zeros((n_states, 2, b))\n",
    "    for s in range(n_states):\n",
    "        for a in range(2):\n",
    "            next_states[s, a] = np.random.choice(n_states, size=b, replace=False)\n",
    "            rewards[s, a] = np.random.normal(0, 1, b)\n",
    "    return r_term, next_states, rewards\n",
    "\n",
    "def evaluate_policy(policy, r_term, next_states, rewards, b, n_states, max_iter=1000, tol=1e-4):\n",
    "    \"\"\"Evaluate the policy using iterative policy evaluation.\"\"\"\n",
    "    V_old = np.zeros(n_states)\n",
    "    for it in range(max_iter):\n",
    "        V_new = np.zeros(n_states)\n",
    "        for s in range(n_states):\n",
    "            a = policy[s]\n",
    "            term_val = 0.1 * r_term[s, a]\n",
    "            ns = next_states[s, a]\n",
    "            r_non = rewards[s, a]\n",
    "            non_term_val = 0.9 / b * (np.sum(r_non) + np.sum(V_old[ns]))\n",
    "            V_new[s] = term_val + non_term_val\n",
    "        max_diff = np.max(np.abs(V_new - V_old))\n",
    "        if max_diff < tol:\n",
    "            break\n",
    "        V_old = V_new\n",
    "    return V_new\n",
    "\n",
    "def run_uniform(r_term, next_states, rewards, b, n_states, max_updates, eval_interval):\n",
    "    \"\"\"Perform uniform updates over state-action pairs.\"\"\"\n",
    "    n_actions = 2\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    sa_pairs = [(s, a) for s in range(n_states) for a in range(n_actions)]\n",
    "    total_sa = len(sa_pairs)\n",
    "    values = []\n",
    "    for update_step in range(max_updates):\n",
    "        s, a = sa_pairs[update_step % total_sa]\n",
    "        term_r = r_term[s, a]\n",
    "        ns = next_states[s, a]\n",
    "        r_non = rewards[s, a]\n",
    "        max_Qs = np.max(Q[ns], axis=1)\n",
    "        non_term_val = 0.9 / b * (np.sum(r_non) + np.sum(max_Qs))\n",
    "        Q[s, a] = 0.1 * term_r + non_term_val\n",
    "        if (update_step + 1) % eval_interval == 0:\n",
    "            policy = np.argmax(Q, axis=1)\n",
    "            V = evaluate_policy(policy, r_term, next_states, rewards, b, n_states)\n",
    "            values.append(V[0])\n",
    "    return values\n",
    "\n",
    "def run_onpolicy(r_term, next_states, rewards, b, n_states, max_updates, eval_interval, start_state=0):\n",
    "    \"\"\"Perform on-policy updates during simulated episodes.\"\"\"\n",
    "    n_actions = 2\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    values = []\n",
    "    updates = 0\n",
    "    while updates < max_updates:\n",
    "        state = start_state\n",
    "        terminated = False\n",
    "        while not terminated and updates < max_updates:\n",
    "            if np.random.rand() < 0.1:\n",
    "                action = np.random.randint(0, n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            term_r = r_term[state, action]\n",
    "            ns = next_states[state, action]\n",
    "            r_non = rewards[state, action]\n",
    "            max_Qs = np.max(Q[ns], axis=1)\n",
    "            non_term_val = 0.9 / b * (np.sum(r_non) + np.sum(max_Qs))\n",
    "            Q[state, action] = 0.1 * term_r + non_term_val\n",
    "            if np.random.rand() < 0.1:\n",
    "                terminated = True\n",
    "            else:\n",
    "                state = np.random.choice(ns)\n",
    "            updates += 1\n",
    "            if updates % eval_interval == 0:\n",
    "                policy = np.argmax(Q, axis=1)\n",
    "                V = evaluate_policy(policy, r_term, next_states, rewards, b, n_states)\n",
    "                values.append(V[0])\n",
    "    return values\n",
    "\n",
    "# Parameters\n",
    "n_states = 10000\n",
    "max_updates = 1000\n",
    "eval_interval = 50\n",
    "n_tasks = 30\n",
    "branching_factors = [1, 3]\n",
    "methods = ['uniform', 'onpolicy']\n",
    "results = {b: {method: np.zeros(max_updates // eval_interval) for method in methods} for b in branching_factors}\n",
    "\n",
    "# Main experiment loop\n",
    "for b in branching_factors:\n",
    "    print(f\"Branching factor b = {b}\")\n",
    "    avg_uniform = np.zeros(max_updates // eval_interval)\n",
    "    avg_onpolicy = np.zeros(max_updates // eval_interval)\n",
    "    \n",
    "    for task_idx in tqdm(range(n_tasks)):\n",
    "        r_term, next_states, rewards = generate_task(n_states, b, seed=task_idx)\n",
    "        \n",
    "        # Uniform updates\n",
    "        uniform_vals = run_uniform(r_term, next_states, rewards, b, n_states, max_updates, eval_interval)\n",
    "        avg_uniform += np.array(uniform_vals)\n",
    "        \n",
    "        # On-policy updates\n",
    "        onpolicy_vals = run_onpolicy(r_term, next_states, rewards, b, n_states, max_updates, eval_interval)\n",
    "        avg_onpolicy += np.array(onpolicy_vals)\n",
    "    \n",
    "    avg_uniform /= n_tasks\n",
    "    avg_onpolicy /= n_tasks\n",
    "    results[b]['uniform'] = avg_uniform\n",
    "    results[b]['onpolicy'] = avg_onpolicy\n",
    "\n",
    "# Plot results\n",
    "x = np.arange(eval_interval, max_updates + 1, eval_interval)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x, results[1]['uniform'], label='Uniform', color='blue')\n",
    "plt.plot(x, results[1]['onpolicy'], label='On-policy', color='red')\n",
    "plt.title('Branching Factor $b=1$ (10,000 states)')\n",
    "plt.ylabel('Value of Start State')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x, results[3]['uniform'], label='Uniform', color='blue')\n",
    "plt.plot(x, results[3]['onpolicy'], label='On-policy', color='red')\n",
    "plt.title('Branching Factor $b=3$ (10,000 states)')\n",
    "plt.xlabel('Number of Expected Updates')\n",
    "plt.ylabel('Value of Start State')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure_8_8_replication.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
