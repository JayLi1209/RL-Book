{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racetrack Problem\n",
    "From Exercise 5.12.\n",
    "This implementation is adapted from [here](https://towardsdatascience.com/solving-reinforcement-learning-racetrack-exercise-building-the-environment-33712602de0c/). This is somewhat similar to the structure of `gym_practice.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from race_track import RaceTrack\n",
    "from scipy.ndimage import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Off-policy Monte Carlo Control part ######\n",
    "\n",
    "# 90% choose the greedy act, 10% explore\n",
    "def behavior_pi(state: tuple, nA: int, target_pi: any, epsilon: float) -> tuple:\n",
    "    rand_val = np.random.rand()\n",
    "    greedy_act = target_pi[state]\n",
    "\n",
    "    if rand_val > epsilon:\n",
    "        return greedy_act, (1 - epsilon + epsilon / nA)\n",
    "    else:\n",
    "        action = np.random.choice(nA)\n",
    "        if action == greedy_act:\n",
    "            return action, (1 - epsilon + epsilon / nA)\n",
    "        else:\n",
    "            return action, epsilon/nA\n",
    "\n",
    "\n",
    "def off_policy_monte_carlo(total_episodes:int, track_map:str, render_mode:str, zero_acc:bool=False) -> tuple:\n",
    "    gamma = 0.9 # arbitrary\n",
    "    epsilon = 0.1 # arbitrary\n",
    "\n",
    "    env = RaceTrack(track_map, render_mode, size=20)\n",
    "    action_space = env.nA # (9, ) nine actions\n",
    "    observation_space = env.nS # (row, col, cur_speed_x, cur_speed_y)\n",
    "\n",
    "    # Initialize action values (Q) and count (C)\n",
    "    Q = np.random.normal(size=(*observation_space, action_space))\n",
    "    Q -= 500 # if we initialize Q with mean 0, var 1, then it is too optimistic because \n",
    "    # at start, the algoritm will be full of negative values.\n",
    "    C = np.zeros_like(Q)\n",
    "    target_pi = np.argmax(Q, axis=-1)  # the index of the largest action\n",
    "    reward_hist = np.zeros(shape=(total_episodes), dtype=np.float32)\n",
    "\n",
    "    for i in range(total_episodes):\n",
    "        trajectory = []\n",
    "        terminated = False\n",
    "        state, info = env.reset()\n",
    "        (action, act_prob) = behavior_pi(state, env.nA, target_pi, epsilon)\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        # print(\"Sampling b\")\n",
    "        # Sample from using b\n",
    "        while not terminated:\n",
    "            if zero_acc and np.random.rand() <= 0.1:\n",
    "                observation, reward, terminated, _ = env.step(4)\n",
    "            else:\n",
    "                observation, reward, terminated, _ = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            trajectory.append((state, action, reward, act_prob))\n",
    "            state = observation\n",
    "            (action, act_prob) = behavior_pi(state, env.nA, target_pi, epsilon)\n",
    "\n",
    "\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        while trajectory:\n",
    "            (state, action, reward, act_prob) = trajectory.pop()\n",
    "            G = gamma * G + reward\n",
    "            C[state][action] = C[state][action] + W\n",
    "            Q[state][action] = Q[state][action] + (W/C[state][action]) * (G - Q[state][action])\n",
    "\n",
    "            target_pi[state] = np.argmax(Q[state])\n",
    "            if action != target_pi[state]:\n",
    "                break\n",
    "            W = W * (1 / act_prob)\n",
    "\n",
    "\n",
    "        reward_hist[i] = total_reward\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Episode: {i}, reward: {total_reward}, epsilon: {epsilon}')\n",
    "\n",
    "    return reward_hist, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(value_hist:dict, total_episodes) -> None:\n",
    "    \n",
    "    line_width = 1.2\n",
    "    fontdict = {'fontsize': 12, 'fontweight': 'bold'}\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=150)\n",
    "    plt.ylim((-500.0, 0.0))\n",
    "    plt.grid(c='lightgray')\n",
    "    plt.margins(0.02)\n",
    "\n",
    "    # Draw/remove axis lines\n",
    "    for i, spine in enumerate(plt.gca().spines.values()):\n",
    "        if i in [0, 2]:\n",
    "            spine.set_linewidth(1.5)\n",
    "            continue\n",
    "        spine.set_visible(False)\n",
    "    \n",
    "    x = np.arange(total_episodes)\n",
    "    plt.xscale('log')\n",
    "    plt.xticks([1, 10, 100, 1000, 10_000, 100_000, 1_000_000], \n",
    "               ['1', '10', '100', '1000', '10,000', '100,000', '1,000,000'])\n",
    "\n",
    "    colors = ['tomato', 'cornflowerblue']\n",
    "    for i, (key, value) in enumerate(value_hist.items()):\n",
    "        title, label = key.split(',')\n",
    "        plt.plot(x, uniform_filter(value, size=20), \n",
    "                 linewidth=line_width, \n",
    "                 label=label,\n",
    "                 c=colors[i],\n",
    "                 alpha=0.95)\n",
    "\n",
    "    plt.title(title + ' training record', fontdict=fontdict)\n",
    "    plt.xlabel('Episodes (log scale)', fontdict=fontdict)\n",
    "    plt.ylabel('Rewards', fontdict=fontdict)    \n",
    "    plt.legend()\n",
    "    plt.savefig(f'./tracks/{\"_\".join(title.lower().split())}.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train = True # Switch between train and evaluation\n",
    "    track_sel = 'a'\n",
    "    total_episodes = 100_000\n",
    "  \n",
    "    if train:\n",
    "        reward_hist_dict = dict()\n",
    "        Q_dict = dict()\n",
    "\n",
    "        for i in range(2):\n",
    "            track_name = f'Track {track_sel.capitalize()}'\n",
    "            use_zero_acc = 'with zero acc.' if i else 'without zero acc.'\n",
    "            key = track_name + ',' + use_zero_acc\n",
    "\n",
    "            # print(\"Entering MC\")\n",
    "            reward_hist, Q = off_policy_monte_carlo(total_episodes, track_sel, None, i)\n",
    "            # print(\"Exiting MC\")\n",
    "            reward_hist_dict[key] = reward_hist\n",
    "            Q_dict[key] = Q\n",
    "        \n",
    "        plot_result(reward_hist_dict, total_episodes)\n",
    "        with open(f'./history/exercise_5_12/track_{track_sel}.pkl', 'wb') as f:\n",
    "            pickle.dump(Q_dict, f)\n",
    "\n",
    "    else: # Evaluate the Q values and plot sample paths\n",
    "\n",
    "        with open(f'./history/exercise_5_12/track_{track_sel}.pkl', 'rb') as f:\n",
    "            Q_dict = pickle.load(f)\n",
    "\n",
    "        key = list(Q_dict.keys())[0]\n",
    "        Q = Q_dict[key]\n",
    "        policy = np.argmax(Q, axis=-1) # greedy policy\n",
    "        \n",
    "        env = RaceTrack(track_sel, None, 20)\n",
    "        fig = plt.figure(figsize=(12, 5), dpi=150)\n",
    "        fig.suptitle('Sample trajectories', size=12, weight='bold')\n",
    "\n",
    "        for i in range(10):\n",
    "            track_map = np.copy(env.track_map)\n",
    "            state, obs = env.reset()\n",
    "            terminated = False\n",
    "            \n",
    "            while not terminated:\n",
    "                track_map[state[0], state[1]] = 0.6 \n",
    "                action = policy[state]\n",
    "                next_state, reward, terminated = env.step(action)\n",
    "                state = next_state\n",
    "\n",
    "            ax = plt.subplot(2, 5, i + 1)\n",
    "            ax.axis('off')\n",
    "            ax.imshow(track_map, cmap='GnBu')\n",
    "           \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./tracks/track_{track_sel}_paths.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST CODE FOR THE RANDOM POLICY PROGRAM\n",
    "\n",
    "agent = RaceTrack('b', 'human')\n",
    "\n",
    "agent.reset()\n",
    "observation, reward, terminated, truncated = agent.step(8)\n",
    "print(\"Initial Observation: \", observation[0], observation[1], \" with speed: \", observation[2], observation[3])\n",
    "\n",
    "while True:\n",
    "    observation, reward, terminated, truncated = agent.step(np.random.randint(0,9))\n",
    "    print(\"At: \", observation[0], observation[1], \" with speed: \", observation[2], observation[3])\n",
    "    if terminated == True:\n",
    "        agent.reset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
